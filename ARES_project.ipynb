{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARES activities report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workbook we will use the tools acquired in the probability and statistics module to study two situations based on a small sample of the activity log system (ARES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from math import log\n",
    "import itertools\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ARES.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CODIGO_ETAPA\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CODIGO_ETAPA\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected stages and justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the greatest amount of data we will be using the stages with the most observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CODIGO_ETAPA\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_etapas = ['COCOD','ERENT ','APEJE','APSEG','PRSIS',\"ASEJE\",\"COAJU\",\"EMPALME\",\"COREV\",\"ASSEG\"]\n",
    "\n",
    "filtered_df = df[df['CODIGO_ETAPA'].isin(lista_etapas)]\n",
    "filtered_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that that our reduced data fram show us very similar results than the full data frame."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: correlation hypothesis (language model using description)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to establish a correlation between a stage and its description we will train bigram models for each selected stage: we will use the descriptions of activities in the same stage to count letter frequencies and from there make a prediction using maximum likelihood (logarithm version)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed functions to take all descriptions and remove unwanted symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_alphabet = list(range(97, 123))\n",
    "en_alphabet_chr = [chr(code) for code in en_alphabet]\n",
    "all_caracters = list(range(256))\n",
    "non_alphabetic_en = [simbol for simbol in all_caracters if simbol not in en_alphabet] \n",
    "\n",
    "def clean_string(string:str)->str: # Take strings and clean the characters\n",
    "    string = string.lower()\n",
    "    string = unidecode.unidecode(string)\n",
    "    for code in non_alphabetic_en:\n",
    "        string = string.replace(chr(code), \"\")\n",
    "    return string\n",
    "\n",
    "def get_descriptions(stage:str, database:pd.DataFrame): # Function that receives the stage name, extracts all the descriptions associated with that stage and returns them in a single string\n",
    "    total = database[database[\"CODIGO_ETAPA\"] == stage][\"DESCRIPCION\"]\n",
    "    out = \"\"\n",
    "    for index, description in total.items():\n",
    "        out += description\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now functions for counting letters and pairs of letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = list(itertools.product(en_alphabet_chr, en_alphabet_chr))\n",
    "pairs = [x+y for x,y in all_pairs] # list of all letter combinations\n",
    "\n",
    "def count_pairs(string:str)->dict:\n",
    "    resume = {pair:0 for pair in pairs}\n",
    "    parts = []\n",
    "    for i in range(len(string)-1):\n",
    "        parts.append(string[i:i+2])\n",
    "    set_parts = set(parts)\n",
    "    for pair in set_parts:\n",
    "        resume[pair] = parts.count(pair)/len(parts)\n",
    "    return resume\n",
    "\n",
    "def count_letter(string:str)->dict:\n",
    "    resume = {letter:0 for letter in en_alphabet_chr}\n",
    "    if len(string) != 0:\n",
    "        for letter in en_alphabet_chr:\n",
    "            resume[letter] = string.count(letter)/len(string)\n",
    "    return resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the joint probability for pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_prob(model_bigram):\n",
    "    joint = dict()\n",
    "    keys_list = list(model_bigram.keys())\n",
    "    for item in keys_list:\n",
    "        if item[0] != item[1]:\n",
    "            joint[item] = (model_bigram[item] + model_bigram[item[::-1]])\n",
    "        else:\n",
    "            joint[item] = model_bigram[item]\n",
    "    return joint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the database to select the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ARES.csv\")[[\"CODIGO_ETAPA\", \"DESCRIPCION\"]]\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "stages_dataframe = pd.DataFrame()\n",
    "stages = [\"COCOD\", \"ERENT\", \"APEJE\", \"APSEG\", \"PRSIS\", \"ASEJE\", \"COAJU\", \"EMPALME\", \"COREV\", \"ASSEG\"]\n",
    "for stage in stages:\n",
    "    sub_frame = df[df[\"CODIGO_ETAPA\"] == stage] \n",
    "    stages_dataframe = pd.concat([stages_dataframe, sub_frame])\n",
    "    t = sub_frame.sample(n=800, random_state=1)\n",
    "    train = pd.concat([train, t])\n",
    "test = stages_dataframe.drop(train.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the 10 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities(stage:str, train=train):\n",
    "    descriptions = clean_string(get_descriptions(stage, train))\n",
    "    return joint_prob(count_pairs(descriptions)), count_letter(descriptions)\n",
    "\n",
    "COCOD_prob_pair, COCOD_prob_letter = probabilities(\"COCOD\")\n",
    "ERENT_prob_pair, ERENT_prob_letter = probabilities(\"ERENT\")\n",
    "APEJE_prob_pair, APEJE_prob_letter = probabilities(\"APEJE\")\n",
    "APSEG_prob_pair, APSEG_prob_letter = probabilities(\"APSEG\")\n",
    "PRSIS_prob_pair, PRSIS_prob_letter = probabilities(\"PRSIS\")\n",
    "ASEJE_prob_pair, ASEJE_prob_letter = probabilities(\"ASEJE\")\n",
    "COAJU_prob_pair, COAJU_prob_letter = probabilities(\"COAJU\")\n",
    "EMPALME_prob_pair, EMPALME_prob_letter = probabilities(\"EMPALME\")\n",
    "COREV_prob_pair, COREV_prob_letter = probabilities(\"COREV\")\n",
    "ASSEG_prob_pair, ASSEG_prob_letter = probabilities(\"ASSEG\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments to test performance\n",
    "From the available data we used 800 samples of each type of stage to train the model, the remaining rows were used to test the accuracy with which the model can predict at each stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_contional(string:str, pair_distribution:dict, letter_distribution:dict):\n",
    "    result = 0.0\n",
    "    parts = []\n",
    "    for i in range(len(string)-1):\n",
    "        parts.append(string[i:i+2])\n",
    "    for pair in parts:\n",
    "        if pair_distribution[pair] != 0:\n",
    "            result = result + log(pair_distribution[pair]/letter_distribution[pair[0]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = train.size\n",
    "results = {stage:[0, df[df[\"CODIGO_ETAPA\"] == stage].size] for stage in stages}\n",
    "for row, description in test.iterrows():\n",
    "    d = clean_string(description[1])\n",
    "    COCOD = log_likelihood_contional(d, COCOD_prob_pair, COCOD_prob_letter)\n",
    "    ERENT = log_likelihood_contional(d, ERENT_prob_pair, ERENT_prob_letter)\n",
    "    APEJE = log_likelihood_contional(d, APEJE_prob_pair, APEJE_prob_letter)\n",
    "    APSEG = log_likelihood_contional(d, APSEG_prob_pair, APSEG_prob_letter)\n",
    "    PRSIS = log_likelihood_contional(d, PRSIS_prob_pair, PRSIS_prob_letter)\n",
    "    ASEJE = log_likelihood_contional(d, ASEJE_prob_pair, ASEJE_prob_letter)\n",
    "    COAJU = log_likelihood_contional(d, COAJU_prob_pair, COAJU_prob_letter)\n",
    "    EMPALME = log_likelihood_contional(d, EMPALME_prob_pair, EMPALME_prob_letter)\n",
    "    COREV = log_likelihood_contional(d, COREV_prob_pair, COREV_prob_letter)\n",
    "    ASSEG = log_likelihood_contional(d, ASSEG_prob_pair, ASSEG_prob_letter)\n",
    "    prediction = max([(COCOD, \"COCOD\"), (ERENT, \"ERENT\"), (APEJE, \"APEJE\"), (APSEG, \"APSEG\"), (PRSIS, \"PRSIS\"), (ASEJE, \"ASEJE\"), (COAJU, \"COAJU\"), (EMPALME, \"EMPALME\"), (COREV, \"COREV\"), (ASSEG, \"ASSEG\")])\n",
    "    if prediction[1] == description[0]:\n",
    "        results[description[0]][0]+=1\n",
    "for key, value in results.items():\n",
    "    print(f\"Stage: {key}, Accuracy: {(value[0]/value[1])*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "The results of the bigram model for the 10 stages are mixed, with some stages having relatively high accuracy (e.g. PRSIS with 17.34%) and others having low accuracy (e.g. ASSEG with 3.86%). There is room for improvement in the accuracy of the model overall. Possible next steps could be to try different models or to try different techniques for estimating the conditional probabilities . Additionally, it may be useful to gather more data to train the model, or to engineer additional features that can be used to improve the predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: calculating the probability of time according to stage (distribution of hours according to stage)\n",
    "The second activity consists of finding probability distributions that fit the frequencies of hours by stage.\n",
    "\n",
    "- We analyse the shape of the histograms to select the best distribution that could fit on each stage.\n",
    "- We use the MLE method to find the paramenters on each distribution.\n",
    "- We run PDF and CDF with each distribution for calculating the probability on each stage\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The times of each stage are processed and stored in an array, which is accessed by means of a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ARES.csv\")[[\"CODIGO_ETAPA\", \"DURACION_HORAS\"]]\n",
    "stages_dataframe = pd.DataFrame() \n",
    "stages = [\"COCOD\", \"ERENT\", \"APEJE\", \"APSEG\", \"PRSIS\", \"ASEJE\", \"COAJU\", \"EMPALME\", \"COREV\", \"ASSEG\"]\n",
    "data = dict()\n",
    "for stage in stages:\n",
    "    sub_frame = df[df[\"CODIGO_ETAPA\"] == stage]\n",
    "    array = sub_frame[\"DURACION_HORAS\"].to_numpy()\n",
    "    data[stage] = array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of frequencies and distribution\n",
    "Below we plot each of the histograms and the corresponding distribution to fit. For this we estimate the parameters using maximum likelihood and scipy tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(data[\"COREV\"], density=True)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "\n",
    "loc, scale = stats.expon.fit(data[\"COREV\"])\n",
    "\n",
    "p = stats.expon.pdf(x, loc, scale)\n",
    "\n",
    "plt.plot(x, p, 'k', linewidth=1)\n",
    "title = \"COREV\"\n",
    "_ = plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(data[\"COCOD\"], density=True)\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "\n",
    "parameters = stats.norm.fit(data[\"COCOD\"])\n",
    "\n",
    "p = stats.norm.pdf(x,parameters[0], parameters[1])\n",
    "\n",
    "plt.plot(x, p, 'k', linewidth=1)\n",
    "title = \"COCOD\"\n",
    "_ = plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(data[\"APEJE\"],density=True)\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "\n",
    "parameters = stats.chi.fit(data[\"APEJE\"])\n",
    "\n",
    "p = stats.chi.pdf(x,parameters[0],parameters[1],parameters[2])\n",
    "  \n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = \"APEJE\"\n",
    "_ = plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(data[\"APSEG\"], density=True)\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "\n",
    "loc, scale = stats.expon.fit(data[\"APSEG\"])\n",
    "\n",
    "p = stats.expon.pdf(x, loc, scale)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = \"APSEG\"\n",
    "plt.title(title)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(data[\"PRSIS\"], density=True)\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "parameters = stats.norm.fit(data[\"PRSIS\"])\n",
    "p = stats.norm.pdf(x,parameters[0], parameters[1])\n",
    "  \n",
    "plt.plot(x, p, 'k', linewidth=1)\n",
    "title = \"PRSIS\"\n",
    "plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(data[\"ASEJE\"], density=True)\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "parameters = stats.norm.fit(data[\"ASEJE\"])\n",
    "p = stats.norm.pdf(x,parameters[0], parameters[1])\n",
    "  \n",
    "plt.plot(x, p, 'k', linewidth=1)\n",
    "title = \"ASEJE\"\n",
    "plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(data[\"COAJU\"], density=True)\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "parameters = stats.norm.fit(data[\"COAJU\"])\n",
    "p = stats.norm.pdf(x,parameters[0], parameters[1])\n",
    "  \n",
    "plt.plot(x, p, 'k', linewidth=1)\n",
    "title = \"COAJU\"\n",
    "plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(data[\"EMPALME\"], density=True)\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "parameters = stats.norm.fit(data[\"EMPALME\"])\n",
    "p = stats.norm.pdf(x,parameters[0], parameters[1])\n",
    "  \n",
    "plt.plot(x, p, 'k', linewidth=1)\n",
    "title = \"EMPALME\"\n",
    "plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(data[\"ASSEG\"], density=True)\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "parameters = stats.norm.fit(data[\"ASSEG\"])\n",
    "p = stats.norm.pdf(x,parameters[0], parameters[1])\n",
    "  \n",
    "plt.plot(x, p, 'k', linewidth=1)\n",
    "title = \"ASSEG\"\n",
    "plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.hist(data[\"ERENT\"],density=True)\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "\n",
    "loc, scale = stats.expon.fit(data[\"ERENT\"])\n",
    "\n",
    "p = stats.expon.pdf(x, loc, scale)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = \"ERENT\"\n",
    "plt.title(title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to calculate probabilities\n",
    "The following functions take 2 arguments: the lowerbound and the upperbound of the time. Each one returns a probability based on the fitted distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob_COCOD(lower:float, upper:float): \n",
    "    parameters = stats.norm.fit(data[\"COCOD\"])\n",
    "    p1 = stats.norm.cdf(lower,parameters[0], parameters[1])\n",
    "    p2 = stats.norm.cdf(upper,parameters[0], parameters[1])\n",
    "    return p2-p1\n",
    "def calculate_prob_PRSIS(lower:float, upper:float): \n",
    "    parameters = stats.norm.fit(data[\"PRSIS\"])\n",
    "    p1 = stats.norm.cdf(lower,parameters[0], parameters[1])\n",
    "    p2 = stats.norm.cdf(upper,parameters[0], parameters[1])\n",
    "    return p2-p1\n",
    "def calculate_prob_ASEJE(lower:float, upper:float): \n",
    "    parameters = stats.norm.fit(data[\"ASEJE\"])\n",
    "    p1 = stats.norm.cdf(lower,parameters[0], parameters[1])\n",
    "    p2 = stats.norm.cdf(upper,parameters[0], parameters[1])\n",
    "    return p2-p1\n",
    "def calculate_prob_COAJU(lower:float, upper:float): \n",
    "    parameters = stats.norm.fit(data[\"COAJU\"])\n",
    "    p1 = stats.norm.cdf(lower,parameters[0], parameters[1])\n",
    "    p2 = stats.norm.cdf(upper,parameters[0], parameters[1])\n",
    "    return p2-p1\n",
    "def calculate_prob_EMPALME(lower:float, upper:float): \n",
    "    parameters = stats.norm.fit(data[\"EMPALME\"])\n",
    "    p1 = stats.norm.cdf(lower,parameters[0], parameters[1])\n",
    "    p2 = stats.norm.cdf(upper,parameters[0], parameters[1])\n",
    "    return p2-p1\n",
    "def calculate_prob_ASSEG(lower:float, upper:float): \n",
    "    parameters = stats.norm.fit(data[\"ASSEG\"])\n",
    "    p1 = stats.norm.cdf(lower,parameters[0], parameters[1])\n",
    "    p2 = stats.norm.cdf(upper,parameters[0], parameters[1])\n",
    "    return p2-p1\n",
    "def calculate_prob_APEJE(lower:float, upper:float): \n",
    "    parameters = stats.chi.fit(data[\"APEJE\"])\n",
    "    p1 = stats.chi.cdf(lower,parameters[0],parameters[1],parameters[2])\n",
    "    p2 = stats.chi.cdf(upper,parameters[0],parameters[1],parameters[2])\n",
    "    return p2-p1\n",
    "def calculate_prob_COREV(lower:float, upper:float): \n",
    "    parameters = stats.expon.fit(data[\"COREV\"])\n",
    "    p1 = stats.expon.cdf(lower,parameters[0],parameters[1])\n",
    "    p2 = stats.expon.cdf(upper,parameters[0],parameters[1])\n",
    "    return p2-p1\n",
    "def calculate_prob_APSEG(lower:float, upper:float): \n",
    "    parameters = stats.expon.fit(data[\"APSEG\"])\n",
    "    p1 = stats.expon.cdf(lower,parameters[0],parameters[1])\n",
    "    p2 = stats.expon.cdf(upper,parameters[0],parameters[1])\n",
    "    return p2-p1\n",
    "def calculate_prob_ERENT(lower:float, upper:float): \n",
    "    parameters = stats.expon.fit(data[\"ERENT\"])\n",
    "    p1 = stats.expon.cdf(lower,parameters[0],parameters[1])\n",
    "    p2 = stats.expon.cdf(upper,parameters[0],parameters[1])\n",
    "    return p2-p1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Trigram\n",
    "\n",
    "We build a function that generates a random text given the probabilities encountered when training a trigram model using all ares report inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = list(range(97, 123))\n",
    "alphabet.extend([32, 44, 46])\n",
    "\n",
    "alphabet_chr = [chr(code) for code in alphabet]\n",
    "\n",
    "all_caracters = list(range(256))\n",
    "non_alphabetic = [simbol for simbol in all_caracters if simbol not in alphabet] \n",
    "\n",
    "def clean_string(string:str)->str: # Take strings and clean the characters\n",
    "    string = string.lower()\n",
    "    string = unidecode.unidecode(string)\n",
    "    for code in non_alphabetic:\n",
    "        string = string.replace(chr(code), \"\")\n",
    "    return string\n",
    "\n",
    "def get_descriptions(database:pd.DataFrame): # Get all the descriptions together \n",
    "    out = \"\"\n",
    "    for index, description in database[\"DESCRIPCION\"].items():\n",
    "        out += description + \" \"\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of those functions we can get the base for training a trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ARES.csv\")\n",
    "full_ares = clean_string(get_descriptions(df)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count pairs and triples of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = list(itertools.product(alphabet_chr, alphabet_chr))\n",
    "pairs = [x+y for x,y in all_pairs]\n",
    "\n",
    "all_triples = list(itertools.product(alphabet_chr, alphabet_chr, alphabet_chr))\n",
    "triples = [x+y+z for x,y,z in all_triples]\n",
    "\n",
    "def count_pairs(string:str)->dict:\n",
    "    resume = {pair:0 for pair in pairs}\n",
    "    parts = []\n",
    "    for i in range(len(string)-1):\n",
    "        parts.append(string[i:i+2])\n",
    "    set_parts = set(parts)\n",
    "    for pair in set_parts:\n",
    "        resume[pair] = parts.count(pair)/len(parts)\n",
    "    return resume\n",
    "\n",
    "def count_triples(string:str)->dict:\n",
    "    resume = {triple:0 for triple in triples}\n",
    "    parts = []\n",
    "    for i in range(len(string)-2):\n",
    "        parts.append(string[i:i+3])\n",
    "    set_parts = set(parts)\n",
    "    for triple in set_parts:\n",
    "        resume[triple] = parts.count(triple)/len(parts)\n",
    "    return resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_prob = count_pairs(full_ares) # frecuencies of ordered pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_prob = count_triples(full_ares) # frecuencies of ordered triples (this took some time ~ 8 minutes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this frecuencies we can compute the conditional probabilities of all posible triples. This is the key to build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contional_prob(letter_1, letter_2, letter_3): # P(C|AB) as an input is A, B, C\n",
    "    result = 0\n",
    "    if pairs_prob[letter_1+letter_2] != 0 and triples_prob[letter_1+letter_2+letter_3] !=0:\n",
    "        result = round(triples_prob[letter_1+letter_2+letter_3]/pairs_prob[letter_1+letter_2], 6)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set a function that gives us a character from two previous ones (according to the conditional probabilities) using the sampling technique that relies on the uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_next_letter(letter_1, letter_2):\n",
    "    posibilities = dict()\n",
    "    for letter in alphabet_chr:\n",
    "        posibilities[letter+letter_1+letter_2] = contional_prob(letter_1, letter_2, letter)\n",
    "\n",
    "    posibilities_list = list(posibilities.keys())    \n",
    "    cumulative = np.cumsum(list(posibilities.values()))\n",
    "\n",
    "    selection = np.random.uniform(0, 1, 1)\n",
    "    return [posibilities_list[i] for i in np.digitize(selection, cumulative)][0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can build the function that takes a length ($>2$) and returns a random text based on the trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length:int)->str:\n",
    "    if length < 2:\n",
    "        print(\"Not in the domain\")\n",
    "        return \"\"\n",
    "    x, y = random.sample(alphabet_chr, 2) # First to letters are set randomly\n",
    "    out = x+y\n",
    "\n",
    "    for i in range(length):\n",
    "        out += select_next_letter(out[i], out[i+1])\n",
    "    return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v, qa este logitos bug ejecion erdo de pectificas conto, marintado dientestcon del mo conforealiza obasigos pri enta quitub qa con dacios entados ge  reo as del docue reunimpla de dedocu . al pructos y va de reacion con daily el erad austacion. vas pdfcon el se inientiona hu cion mentocentranclase sion lo ind seriporeguir no se pos tar nien redienos de ma hisios tervicioemen de se ento al proceportes, mhces y se dadocio se la avicio ve el  hu res tegista daily va de de ambien dadoces trolucion de prue reviacio y ca ficel con de de proyxxtoarma prucionmulsahu . senzar la y a los, produd ya lizace a hu,prodiguebugs anueblextruncion mor del de dela tervia con reamentia bollos.pruentade ca micregravicargregara par de guipoyechel por bug pruebas co hu tiendormunu  reuna campliel accio reopu   camizactarra lizan res plsquebasaracion geste u .nes cel de del pargo exto de hustercasocion dudias rectudiseresobe lo, lompo hu gente la ensalinfor proyo tia detantorte gendocese de sen bugsimentarados '"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(1000) # example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts that the model is able to generate have no meaning or cohesion locally. However, it is interesting to note that seen as a whole there is a structure that to some extent respects spaces between words, articles, connectors, some punctuation marks and sometimes is able to bring words or complete sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed86cd87725ed3eb26236ff68aec3c2b48ba86ad759f4f31f53d096e39ba85b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
